## RESUMO


Nesta Sprint utilizamos algumas ferramentas de suma importância no trabalho de engenharia de dados como AWS Glue, AWS Athena, AWS S3, Pyspark, e etc. No desafio tivemos de fazer o processamento da camada Trusted, fazendo a padronização dos dados gerados pelo Lambda (csv) transformando-os em arquivos Parquet e por fim salvando no S3. Ja nos exercicios, geramos um arquivo de aleatórios de animais, e lidamos com dados em massa, adicionando colunas a ele e realizando filtragens.

## Evidências

- Nesta sprint será dado destaque sa evidência da criação dos Parquet's do CSV e JSON nos determinados buckets.

### Arquivos Parquet (CSV) no S3:

![Codigo_criacao_imagem](./../Sprint%208/evidencias/Parquet%20CSV/pos_execucao_job_csv.png)

### Arquivos Parquet (JSON) no S3:

![Codigo_criacao_imagem](./../Sprint%208/evidencias/Parquet%20JSON/04_pos_execucao_json.png)

## Exercícios:

- Segue abaixo a evidência do exercício da geração em massa, no passo 10 (contagem de gerações por países):


![alt text](./../Sprint%208/evidencias/geracao_em_massa_passo_10.png)


- Link para pasta dos exercícios: 

## Certificados



